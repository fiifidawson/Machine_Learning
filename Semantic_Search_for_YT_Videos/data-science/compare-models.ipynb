{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Candidate Search Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.metrics import DistanceMetric\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet('data/video-transcripts.parquet')\n",
    "df_eval = pl.read_csv('data/eval-raw.csv')\n",
    "df.head()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed titles and transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \"parameters\"\n",
    "column_to_embed_list = ['title', 'transcript']\n",
    "model_name_list = [\"all-MiniLM-L6-v2\", \"multi-qa-distilbert-cos-v1\", \"multi-qa-mpnet-base-dot-v1\"]#embedding modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-MiniLM-L6-v2_title\n",
      "CPU times: total: 2.78 s\n",
      "Wall time: 832 ms\n",
      "\n",
      "all-MiniLM-L6-v2_transcript\n",
      "CPU times: total: 47 s\n",
      "Wall time: 9.9 s\n",
      "\n",
      "multi-qa-distilbert-cos-v1_title\n",
      "CPU times: total: 6.14 s\n",
      "Wall time: 1.54 s\n",
      "\n",
      "multi-qa-distilbert-cos-v1_transcript\n",
      "CPU times: total: 2min 4s\n",
      "Wall time: 42.1 s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.6.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9399aee4534c10aa2d40ffa15cc196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  65%|######4   | 283M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7949d3ed504183a4bba1bea184f45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7204c214a88b4f5ba4583bb06cfb6760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1486de6f34742f1afa80397d3c0158e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbabfe53a9c847b2809b8abca33046d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e951c272c0442ab0de7315924e3457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-qa-mpnet-base-dot-v1_title\n",
      "CPU times: total: 12.9 s\n",
      "Wall time: 2.88 s\n",
      "\n",
      "multi-qa-mpnet-base-dot-v1_transcript\n",
      "CPU times: total: 4min 6s\n",
      "Wall time: 1min 23s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate embeddings for each combination of column and model\n",
    "\n",
    "# initialize dict to keep track of all text embeddings\n",
    "text_embedding_dict = {}\n",
    "\n",
    "for model_name in model_name_list:\n",
    "\n",
    "    # define embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    for column_name in column_to_embed_list:\n",
    "\n",
    "        # define text embedding identifier\n",
    "        key_name = model_name + \"_\" + column_name\n",
    "        print(key_name)\n",
    "\n",
    "        # generate embeddings for text under column_name\n",
    "        %time embedding_arr = model.encode(df[column_name].to_list())\n",
    "        print('')\n",
    "\n",
    "        # append embeddings to dict\n",
    "        text_embedding_dict[key_name] = embedding_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adlercohen\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-MiniLM-L6-v2\n",
      "CPU times: total: 4.72 s\n",
      "Wall time: 1.03 s\n",
      "\n",
      "multi-qa-distilbert-cos-v1\n",
      "CPU times: total: 9.97 s\n",
      "Wall time: 3.25 s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.6.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-qa-mpnet-base-dot-v1\n",
      "CPU times: total: 40.2 s\n",
      "Wall time: 8.9 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_embedding_dict = {}\n",
    "\n",
    "for model_name in model_name_list:\n",
    "\n",
    "    # define embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(model_name)\n",
    "\n",
    "    # embed query text\n",
    "    %time embedding_arr = model.encode(df_eval['query'].to_list())\n",
    "    print('')\n",
    "\n",
    "    # append embedding to dict\n",
    "    query_embedding_dict[model_name] = embedding_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate search methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnVideoID_index(df: pl.dataframe.frame.DataFrame, df_eval:  pl.dataframe.frame.DataFrame, query_n: int) -> int:\n",
    "    \"\"\"\n",
    "        Function to return the index of a dataframe corresponding to the nth row in evaluation dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    return [i for i in range(len(df)) if df['video_id'][i]==df_eval['video_id'][query_n]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalTrueRankings(dist_arr_isorted: np.ndarray, df:  pl.dataframe.frame.DataFrame, df_eval:  pl.dataframe.frame.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Fun to return \"true\" video ID rankings for each evaluation query\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize array to store rankings of \"correct\" search result\n",
    "    true_rank_arr = np.empty((1, dist_arr_isorted.shape[1]))\n",
    "\n",
    "    # evaluate ranking of correct result for each query\n",
    "    for query_n in range(dist_arr_isorted.shape[1]):\n",
    "\n",
    "        # return \"true\" video ID's in df\n",
    "        video_id_idx = returnVideoID_index(df, df_eval, query_n)\n",
    "\n",
    "        # evaluate the ranking of the \"true\" video ID\n",
    "        true_rank = np.argwhere(dist_arr_isorted[:,query_n] == video_id_idx)[0][0]\n",
    "\n",
    "        # store the \"true\" video ID's ranking in array\n",
    "        true_rank_arr[0, query_n] = true_rank\n",
    "\n",
    "    return true_rank_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize distance metrics to experiment\n",
    "dist_name_list = ['euclidean', 'manhattan', 'chebyshev']\n",
    "sim_name_list = ['cos_sim', 'dot_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate all possible combinations of model, columns to embed, and distance metrics\n",
    "\n",
    "# initialise list to store results\n",
    "eval_results = []\n",
    "\n",
    "# loop through all models\n",
    "for model_name in model_name_list:\n",
    "\n",
    "    # generate query embedding\n",
    "    query_embedding = query_embedding_dict[model_name]\n",
    "\n",
    "    # loop through text columns\n",
    "    for column_name in column_to_embed_list:\n",
    "\n",
    "        # generate column embedding\n",
    "        embedding_arr = text_embedding_dict[model_name+'_'+column_name]\n",
    "\n",
    "        # loop through distance metrics\n",
    "        for dist_name in dist_name_list:\n",
    "\n",
    "            # compute distance between video text and query\n",
    "            dist = DistanceMetric.get_metric(dist_name)\n",
    "            dist_arr = dist.pairwise(embedding_arr, query_embedding)\n",
    "\n",
    "            # sort indexes of distance array\n",
    "            dist_arr_isorted = np.argsort(dist_arr, axis=0)\n",
    "\n",
    "            # define label for search method\n",
    "            method_name = \"_\".join([model_name, column_name, dist_name])\n",
    "\n",
    "            # evaluate the ranking of the ground truth\n",
    "            true_rank_arr = evalTrueRankings(dist_arr_isorted, df, df_eval)\n",
    "\n",
    "            # store results\n",
    "            eval_list = [method_name] + true_rank_arr.tolist()[0]\n",
    "            eval_results.append(eval_list)\n",
    "\n",
    "        # loop through sbert similarity scores\n",
    "        for sim_name in sim_name_list:\n",
    "\n",
    "            # apply similarity score from sbert\n",
    "            cmd = \"dist_arr = -util.\" + sim_name + \"(embedding_arr, query_embedding)\"\n",
    "            exec(cmd)\n",
    "\n",
    "            # sort indexes of distance array \n",
    "            dist_arr_isorted = np.argsort(dist_arr, axis=0)\n",
    "\n",
    "            # define label for search method\n",
    "            method_name = \"_\".join([model_name, column_name, sim_name.replace(\"_\",\"-\")])\n",
    "\n",
    "            # define the ranking of the ground truth\n",
    "            true_rank_arr = evalTrueRankings(dist_arr_isorted, df, df_eval)\n",
    "\n",
    "            # store results\n",
    "            eval_list = [method_name] + true_rank_arr.tolist()[0]\n",
    "            eval_results.append(eval_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dist_arr = -util.dot_score(embedding_arr, query_embedding)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rankings for title + transcripts embedding\n",
    "for model_name in model_name_list:\n",
    "\n",
    "    # generate embeddings\n",
    "    embedding_arr1 = text_embedding_dict[model_name+'_title']\n",
    "    embedding_arr2 = text_embedding_dict[model_name+'_transcript']\n",
    "    query_embedding = query_embedding_dict[model_name]\n",
    "\n",
    "\n",
    "    for dist_name in dist_name_list:\n",
    "\n",
    "        # compute distance between video text and query\n",
    "        dist = DistanceMetric.get_metric(dist_name)\n",
    "        dist_arr = dist.pairwise(embdding_arr1, query_embedding) + dist.pairwise(embdding_arr2, query_embedding)\n",
    "\n",
    "        # sort indexes of distance array\n",
    "        dist_arr_isorted = mp.argsort(dist_arr, axis=0)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

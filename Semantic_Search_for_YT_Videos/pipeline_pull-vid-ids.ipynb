{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL of YouTube Video Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import requests\n",
    "import json\n",
    "import polars as pl\n",
    "from my_sk import my_key\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getVideoRecords(response: requests.models.Response) -> list:\n",
    "    \"\"\"\n",
    "        Function to extract YouTube video from GET request response\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize list to store data from page results\n",
    "    video_record_list = []\n",
    "\n",
    "    for raw_item in json.loads(response.text)['items']:\n",
    "\n",
    "        # only execute for youtube videos\n",
    "        if raw_item['id']['kind'] != \"youtube#video\":\n",
    "            continue\n",
    "\n",
    "        # extract relevant data\n",
    "        Video_record = {}\n",
    "        Video_record['video_id'] = raw_item['id']['videoId']\n",
    "        Video_record['datetime'] = raw_item['snippet']['publishedAt']\n",
    "        Video_record['title'] = raw_item['snippet']['title']\n",
    "\n",
    "        # append record to list\n",
    "        video_record_list.append(Video_record)\n",
    "\n",
    "    return video_record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(transcript: list) -> str:\n",
    "    \"\"\"\n",
    "        Function to extract text from transcript dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    text_list = [transcript[i]['text'] for i in range(len(transcript))]\n",
    "    return ''.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract videos IDs (+ datetime, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define channel ID\n",
    "channel_id = 'UCa9gErQ9AE5jT2DZLjXBIdA'\n",
    "\n",
    "# define url for API\n",
    "url = 'https://www.googleapis.com/youtube/v3/search'\n",
    "\n",
    "# initialize page token \n",
    "page_token = None\n",
    "\n",
    "# initialize list to store video data\n",
    "video_record_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video data across multiple search result pages\n",
    "\n",
    "while page_token != 0:\n",
    "    # define parameters for API call\n",
    "    params = {'key': my_key, 'channelId': channel_id, 'part': [\"snippet\",\"id\"], 'order':\"date\", 'maxResults':50, 'pageToken':page_token}\n",
    "\n",
    "    # make get request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # append video data from page results to list\n",
    "    video_record_list += getVideoRecords(response)\n",
    "\n",
    "    try: \n",
    "        # get next page token\n",
    "        page_token = json.loads(response.text)['nextPageToken']\n",
    "    except:\n",
    "        # if no next page token, kill while loop\n",
    "        page_token = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data in polars dataframe\n",
    "df = pl.DataFrame(video_record_list)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list to score video captions\n",
    "transcript_text_list = []\n",
    "\n",
    "# loop through each row of dataframe\n",
    "for i in range(len(df)):\n",
    "\n",
    "    # try to extract captions\n",
    "    try:\n",
    "        # get transcript\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(df['video_id'][i])\n",
    "        # extract text transcript\n",
    "        transcript_text = extract_text(transcript)\n",
    "    # if not captions available set as n/a\n",
    "    except:\n",
    "        transcript_text = \"n/a\"\n",
    "\n",
    "    # append transcript text to list \n",
    "    transcript_text_list.append(transcript_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add transcripts to dataframe\n",
    "df = df.with_columns(pl.Series(name=\"transcript\", values=transcript_text_list))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape + unique values\n",
    "print(\"shape:\", df.shape)\n",
    "print(\"n unique rows:\", df.n_unique())\n",
    "for j in range(df.shape[1]):\n",
    "    print(\"n unique elements (\" + df.columns[j] + \"):\", df[:,j].n_unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datetime to Datetime dtype\n",
    "df = df.with_columns(pl.col('datetime').cast(pl.Datetime))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all special strings and their replacements\n",
    "special_strings = ['&#39;', '&amp;', 'sha ']\n",
    "special_strings_replacements = [\"'\", \"&\", \"Shaw \"]\n",
    "\n",
    "# replace each special string appearing in title and transcript columns\n",
    "for i in range(len(special_strings)):\n",
    "    df = df.with_columns(df['title'].str.replace(special_strings[i],\n",
    "                        special_strings_replacements[i]).alias('title'))\n",
    "    df = df.with_columns(df['transcript'].str.replace(special_strings[i],\n",
    "                        special_strings_replacements[i]).alias('transcript'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to file\n",
    "pl.DataFrame(video_record_list).write_parquet('video-ids.parquet')\n",
    "pl.DataFrame(video_record_list).write_csv('video-ids.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
